{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh1SOqFVyuPo"
      },
      "source": [
        "# Análisis de datos exploratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Se debe ejecutar despúes del notebook `data.ipynb`\n",
        "- Este notebook crea el archivo necesario para `export.ipynb`\n",
        "- Los archivos generados son:\n",
        "  - `model.h5`: Modelo de red neuronal (Este archivo es necesario para predecir las new entrantes la app web `./client`)\n",
        "- Es necesario [descargar](https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt) el modelo de Glove de 100 dimensiones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juw3GXrVyuPp"
      },
      "source": [
        "## Cargar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "_Eh75So8yuPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"features.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "evNpSbVeyuPs",
        "outputId": "86460e6e-f786-4f72-b852-e0a929c87211"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>features</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>win court lose ground uncertainty cloud vote r...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>obama name first africanamerican woman librari...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>daughter allege iowa cop killer state father s...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>burr nunes step aside protect russia probe inv...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>postseason bucs top yank fall tiger split six ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            features  label\n",
              "0  win court lose ground uncertainty cloud vote r...      1\n",
              "1  obama name first africanamerican woman librari...      1\n",
              "2  daughter allege iowa cop killer state father s...      0\n",
              "3  burr nunes step aside protect russia probe inv...      1\n",
              "4  postseason bucs top yank fall tiger split six ...      0"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmuowRWiyuPs",
        "outputId": "48ea296c-4a30-4779-89f8-20af101eae22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 76524 entries, 0 to 76523\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype \n",
            "---  ------    --------------  ----- \n",
            " 0   features  76524 non-null  object\n",
            " 1   label     76524 non-null  int64 \n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 1.2+ MB\n"
          ]
        }
      ],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c29vbJyuPw"
      },
      "source": [
        "## Cargando el modelo de Word Embedding (Glove 100d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cargar el modelo de glove\n",
        "def load_glove_model(file_path):\n",
        "    \"\"\"Cargar un modelo GloVe desde un archivo de texto.\"\"\"\n",
        "    model = {}\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        # Parsear el archivo .txt\n",
        "        for line in file:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = [float(value) for value in values[1:]]\n",
        "            model[word] = vector\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cosine_similarity(vec1, vec2):\n",
        "    \"\"\"Calcular la similitud coseno entre dos vectores.\"\"\"\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "    similarity = dot_product / (norm_vec1 * norm_vec2)\n",
        "    return similarity\n",
        "\n",
        "def most_similar_words(word, model, top_n=5):\n",
        "    \"\"\"Encontrar las palabras más similares a la palabra dada en el modelo.\"\"\"\n",
        "    if word not in model:\n",
        "        print(f\"La palabra '{word}' no está en el modelo.\")\n",
        "        return []\n",
        "\n",
        "    word_vector = model[word]\n",
        "    similarities = []\n",
        "\n",
        "    for other_word, other_vector in model.items():\n",
        "        if other_word != word:\n",
        "            similarity = cosine_similarity(word_vector, other_vector)\n",
        "            similarities.append((other_word, similarity))\n",
        "\n",
        "    # Ordenar por similitud descendente\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Devolver las palabras más similares (top_n)\n",
        "    return similarities[:top_n]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Palabras más similares a 'ate':\n",
            "eaten: 0.809029408004442\n",
            "drank: 0.7871671568844296\n",
            "eat: 0.7813820363677394\n",
            "eating: 0.7338559174177061\n",
            "eats: 0.7251667659830515\n",
            "slept: 0.7152498971261759\n",
            "smoked: 0.6635120852825582\n",
            "meal: 0.654029261267389\n",
            "cooked: 0.6502910720528902\n",
            "dined: 0.6021937327293632\n"
          ]
        }
      ],
      "source": [
        "# Ruta al archivo GloVe\n",
        "glove_file_path = 'glove.6B.100d.txt'\n",
        "\n",
        "# Cargar el modelo GloVe\n",
        "embedding_index = load_glove_model(glove_file_path)\n",
        "\n",
        "# Ejemplo de uso con la palabra 'ate'\n",
        "word_to_lookup = 'ate'\n",
        "similar_words = most_similar_words(word_to_lookup, embedding_index, top_n=10)\n",
        "\n",
        "print(f\"Palabras más similares a '{word_to_lookup}':\")\n",
        "for similar_word, similarity in similar_words:\n",
        "    print(f\"{similar_word}: {similarity}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creacion del Word Embedding en base a nuestros datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Variables para el modelo\n",
        "vocab_size = 20000 # Número máximo de palabras a utilizar\n",
        "max_sequence_length = 300 # Número máximo de palabras en una secuencia\n",
        "embedding_dim = 100 # Dimensión del vector de embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenizar la data (BOW)\n",
        "tokenizer = Tokenizer(num_words=vocab_size)  # Instanciamos el tokenizer\n",
        "tokenizer.fit_on_texts(df[\"features\"])  # convert to string type\n",
        "\n",
        "# Obtenemos solo las primeras 10000 palabras\n",
        "tokenizer.word_index = {e:i for e,i in tokenizer.word_index.items() if i <= vocab_size}\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Observamos el tamaño del vocabulario\n",
        "# vocab_size = len(word_index)\n",
        "# vocab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulario invertido guardado en `vocab.json`\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "# Invertimos el vocabulario\n",
        "inv_map = {v: k for k, v in tokenizer.index_word.items()}\n",
        "\n",
        "# Guarda el vocabulario invertido en un archivo JSON\n",
        "with open('vocab.json', 'w') as archivo:\n",
        "    json.dump(inv_map, archivo)\n",
        "\n",
        "# El vocabulario es guardado para posteriormente ser usado en clasificacion usando web scrapping\n",
        "print(f'Vocabulario invertido guardado en `vocab.json`')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convertimos las palabras en secuencias de números\n",
        "sequences = tokenizer.texts_to_sequences(df[\"features\"])\n",
        "\n",
        "# Aplicamos padding a las secuencias\n",
        "padded_seq = pad_sequences(\n",
        "    sequences,\n",
        "    maxlen=max_sequence_length,\n",
        "    padding=\"post\",\n",
        "    truncating=\"post\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el embedding matrix de nuestro dataset\n",
        "embedding_matrix = np.zeros((vocab_size + 1, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i <= vocab_size:\n",
        "        embedding_vector = embedding_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([-0.70878 , -0.038324, -0.91316 ,  0.46671 ,  0.4833  ,  1.0924  ,\n",
              "        0.33082 ,  0.54534 ,  0.68029 , -0.16311 , -0.4366  , -0.051576,\n",
              "        0.84049 ,  0.14467 , -0.12662 , -0.10113 , -0.13787 , -0.18248 ,\n",
              "       -0.067652, -0.37398 ,  1.2349  , -0.047318,  0.45152 ,  0.63501 ,\n",
              "        0.36439 , -0.79032 , -0.47052 ,  0.14723 ,  0.11583 ,  0.48238 ,\n",
              "       -0.11306 ,  0.079177, -0.35294 ,  0.038699, -0.15867 ,  0.42614 ,\n",
              "       -0.71546 , -0.87426 , -0.54662 , -0.14163 , -0.10158 ,  0.9077  ,\n",
              "       -0.95376 ,  0.52829 ,  0.26935 , -0.41898 ,  0.66448 , -0.33511 ,\n",
              "        0.49773 , -0.91425 ,  0.059091,  0.015071,  0.14594 ,  0.8844  ,\n",
              "        0.18616 , -0.27119 , -0.37574 , -0.46423 , -0.40642 ,  0.39076 ,\n",
              "       -0.04388 ,  0.98766 , -0.14428 , -0.089652,  0.06662 ,  0.063583,\n",
              "        0.74573 , -0.60276 ,  0.19722 ,  0.68743 ,  0.20701 , -0.50921 ,\n",
              "        0.4956  , -0.065256, -0.1679  , -0.30023 , -0.14353 , -0.37801 ,\n",
              "        0.055353,  0.10654 ,  0.21028 , -0.60749 , -0.24983 ,  0.22413 ,\n",
              "       -0.12882 ,  0.042421,  0.48087 ,  0.10473 , -0.8903  ,  0.1091  ,\n",
              "       -0.25378 ,  0.32203 , -0.99134 ,  0.40625 , -0.32142 ,  0.39142 ,\n",
              "       -0.6193  ,  0.83458 ,  0.063066,  0.4402  ])"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embedding_matrix[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([  218,   514,   339,  3658,     9,    23,   968,   373,   323,\n",
              "        1058,    22,   795,     9,   331,   212,   743,    53,     3,\n",
              "          92,     5,   388,  2355,  1572,  1197,   339,  3658,   339,\n",
              "         150,   220,  1028,  2849,   331,    31,  4551,  3401,  1940,\n",
              "         115,   144,  3658,  4127,    83,   339,   561,  2970,  1509,\n",
              "        1028,   115,   476,  2460,  3866,   956,  2450,    31,  1986,\n",
              "       15505,    31,  7476,  2450,  4551,  3401,  4733,   193,  1940,\n",
              "        2292,   164,     6,   285,   432,   772,   129,   609,    51,\n",
              "         153,  1544,  2947,  9658,  5947,  2450,   659, 16854,   331,\n",
              "         157,  1052,   877,     3,   339,  4616,   922,   331,   115,\n",
              "        1199,   124,    59,   445,   331,   180,   566,   436,  2500,\n",
              "        2849,  2450,   198,  1719,    16,   198,  1575,  9658,  2272,\n",
              "       16279,   331,   956,  3447,   544, 14577, 10081,   614,  2450,\n",
              "       15505,   705,   301,    77,  2027,    46,  1028,   617,  1407,\n",
              "         609,   129,   151, 16427,  9060,     9,   614,   104,  2450,\n",
              "        4517,   877,   151,   239,    43, 11541,  1821,   231,   979,\n",
              "         147,   893,  1351,    38,    61,   122,   326,    47,    12,\n",
              "          48,   215,   331,  2849,   331,  4112,   879,  4128,    12,\n",
              "        3677,  8658,   373,  4030,     4,   108,  1638,   369,   331,\n",
              "         956,  1364,   331,  3795,  3848,  1729,   254,  1488,  2629,\n",
              "         903,  4128,   636,  2849,   399,   106,  1488,  1354,  2691,\n",
              "         785,   258,  6348,  6480,   331,  1000,  1005,  2272,  2849,\n",
              "        4146,  3881,  1544,   598,  1602,   476,  4126,  2753,  1719,\n",
              "         344,   152,   879, 11141,   973,  3307,   492,    17,  5882,\n",
              "        7883,  7995,    56,   153,  1930,   241,  4146,  9060,   233,\n",
              "       12622,   996,  1390,   442,    31, 15505, 12622,  6480,  1521,\n",
              "        2849,  4112,    16,  1544,   614, 15505,  6625,  6128,  3094,\n",
              "          15,   195,  2462,   705,   170,  7292,   111,    23,     9,\n",
              "         705,   179,  9734,   647,   571,  3001,  4180,   138,   331,\n",
              "        1061,  1145,  3196,  1146,   705, 15505,    36,  4616,  1715,\n",
              "        1719, 16427,  2601,   288,  3152,  8581,   411,  3654,   429,\n",
              "        8180,  7476,  8659,  2450,   123,   442,  1107,   877,   710,\n",
              "        1056,   769,  2241,  2450,  8333,    88,  4776,  1140,    62,\n",
              "         272,     5,  2565])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "padded_seq[6]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Dividir el dataset en train (80%) y test (20%) \n",
        "X_train, X_test, y_train, y_test = train_test_split(padded_seq, df['label'], test_size=0.20, random_state=42, stratify=df['label'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uv5NKAmyuPw"
      },
      "source": [
        "## Redes Neuronales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "AgiVrYgXyuPw"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 300, 100)          2000100   \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 300, 100)          80400     \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 300, 65)           43160     \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 32)                12544     \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2136237 (8.15 MB)\n",
            "Trainable params: 136137 (531.79 KB)\n",
            "Non-trainable params: 2000100 (7.63 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Construir y entrenar la red neuronal\n",
        "model = Sequential()\n",
        "\n",
        "# Capa de Embedding\n",
        "model.add(\n",
        "    Embedding(\n",
        "        input_dim=vocab_size + 1,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_sequence_length,\n",
        "        trainable=False,\n",
        "    )\n",
        ")\n",
        "model.add(LSTM(units=100, return_sequences=True, dropout=0.2))\n",
        "model.add(LSTM(units=65, return_sequences=True, dropout=0.2))\n",
        "model.add(LSTM(units=32, dropout=0.2, recurrent_dropout=0.2))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-fTU1syuPx",
        "outputId": "bbd8aee1-551c-4397-eaaf-928d72ae80ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            " 214/1914 [==>...........................] - ETA: 16:39 - loss: 0.6521 - accuracy: 0.6189"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    np.array(X_train),\n",
        "    y_train,\n",
        "    epochs=5,\n",
        "    batch_size=32,\n",
        "    verbose=True,\n",
        "    validation_data=(np.array(X_test), y_test),\n",
        "    workers=4\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Guardar el modelo\n",
        "model.save(\"modelo.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Ver importancia de palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCWG5D064L0i"
      },
      "outputs": [],
      "source": [
        "%pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRdiuk6LyuPx"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "\n",
        "class_names=['NotFake','IsFake']\n",
        "explainer= LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "def predict_proba(data):\n",
        "  list_tokenized_ex = tokenizer.texts_to_sequences(data)\n",
        "  Ex = pad_sequences(list_tokenized_ex, maxlen=max_sequence_length)\n",
        "  pred=model.predict(Ex)\n",
        "  returnable=[]\n",
        "  for i in pred:\n",
        "    temp=i[0]\n",
        "    returnable.append(np.array([1-temp,temp]))\n",
        "  return np.array(returnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pzin3wcyuPy"
      },
      "outputs": [],
      "source": [
        "print(\"Actual rating\",df['label'][10])\n",
        "explainer.explain_instance(df['features'][10],predict_proba).show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"features\"].describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Frecuencia de Palabras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(df[\"features\"])\n",
        "\n",
        "# Obtenemos las palabras y las frecuencias\n",
        "palabras = vectorizer.get_feature_names_out()\n",
        "frecuencias = texto_features.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sumar las frecuencias de todas las palabras\n",
        "frecuencias_totales = frecuencias.sum(axis=0)\n",
        "\n",
        "# Obtener las palabras más frecuentes y sus frecuencias\n",
        "palabras_mas_frecuentes = [palabras[i] for i in frecuencias_totales.argsort()[::-1][:10]]\n",
        "frecuencias_mas_frecuentes = [frecuencias_totales[i] for i in frecuencias_totales.argsort()[::-1][:10]]\n",
        "\n",
        "# Crear un gráfico de barras horizontal\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(palabras_mas_frecuentes, frecuencias_mas_frecuentes, color='skyblue')\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.ylabel('Palabra')\n",
        "plt.title('Palabras más frecuentes')\n",
        "plt.gca().invert_yaxis()  # Invertir el eje y para mostrar las palabras más frecuentes arriba\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNyaYIQJyuPy"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oovCAhafyuP8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLu3qGYtyuP8"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(np.array(X_test), y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzbg7b2gyuP8"
      },
      "outputs": [],
      "source": [
        "# Predecir con el modelo\n",
        "predictions = model.predict(np.array(X_test))\n",
        "\n",
        "# Convertir las probabilidades en clases\n",
        "predictions = list(map(lambda x: 1 if (x > 0.5) else 0, predictions))\n",
        "\n",
        "# Mostrar el reporte de clasificación\n",
        "cm = confusion_matrix(y_test, predictions, labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwayw6XkyuP9"
      },
      "outputs": [],
      "source": [
        "# Mostrar el reporte de clasificación\n",
        "print(classification_report(y_test, list(predictions), digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb5NX9v0yuP9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
