{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh1SOqFVyuPo"
      },
      "source": [
        "# Generate and Extract Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juw3GXrVyuPp"
      },
      "source": [
        "## Load Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Eh75So8yuPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATASET = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- **TRUE** : `1`\n",
        "- **FAKE** : `0`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Fake and real news dataset](https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset?select=Fake.csv )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "%pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "!kaggle datasets download -d clmentbisaillon/fake-and-real-news-dataset\n",
        "!unzip \"fake-and-real-news-dataset.zip\"\n",
        "!rm \"fake-and-real-news-dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the dataset of the True\n",
        "df = pd.read_csv(\"True.csv\")\n",
        "# Standardize the dataset\n",
        "df[\"features\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "df[\"label\"] = 1\n",
        "df.drop([\"title\", \"text\", \"subject\", \"date\"], axis=1, inplace=True)\n",
        "# Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the dataset of the True\n",
        "df = pd.read_csv(\"Fake.csv\")\n",
        "# Standardize the dataset\n",
        "df[\"features\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "df[\"label\"] = 0\n",
        "df.drop([\"title\", \"text\", \"subject\", \"date\"], axis=1, inplace=True)\n",
        "# Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm *.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [Fake News Dataset](https://data.mendeley.com/datasets/945z9xkc8d/1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !curl -L \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/945z9xkc8d-1.zip\" -o data.zip\n",
        "# !unzip \"data.zip\" && unzip \"Fake News Dataset.zip\"\n",
        "# !rm -r *.zip\n",
        "\n",
        "# main_directory = os.path.join(os.getcwd(),\"Fake News Dataset\")\n",
        "# subdirectories = [x[0] for x in os.walk(main_directory)][1:]\n",
        "# print(main_directory)\n",
        "# dataframes = []\n",
        "# test = []\n",
        "# # Itera a trav√©s de los subdirectorios y archivos train.csv\n",
        "# for subdirectory in subdirectories:\n",
        "#     file_path = os.path.join(subdirectory, \"train.csv\")\n",
        "#     test_path = os.path.join(subdirectory, \"test.csv\")\n",
        "\n",
        "#     # Verifica si el archivo train.csv existe en el subdirectorio\n",
        "#     if os.path.exists(file_path):\n",
        "#         data = pd.read_csv(file_path)\n",
        "#         dataframes.append(data)\n",
        "\n",
        "#     if os.path.exists(test_path):\n",
        "#         data = pd.read_csv(test_path)\n",
        "#         test.append(data)\n",
        "\n",
        "# # Combina los DataFrames en uno solo\n",
        "# train_data = pd.concat(dataframes, ignore_index=True)\n",
        "# test_data = pd.concat(test,ignore_index=True)\n",
        "# !rm -r \"Fake News Dataset\"\n",
        "# # Guarda el DataFrame combinado en un archivo train_combined.csv\n",
        "# train_data.to_csv(\"train.csv\", index=False)\n",
        "# test_data.to_csv(\"test.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [GonzaloA/fake_news](https://huggingface.co/datasets/GonzaloA/fake_news)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!curl -L \"https://huggingface.co/datasets/GonzaloA/fake_news/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet\" -o 0.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the Dataset\n",
        "df = pd.read_parquet(\"0.parquet\")\n",
        "# Standardize the dataset\n",
        "df[\"features\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "df.drop([\"title\", \"text\", \"Unnamed: 0\"], axis=1, inplace=True)\n",
        "df.insert(0, \"features\", df.pop(\"features\"))\n",
        "# Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm *.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [ErfanMoosaviMonazzah/fake-news-detection-dataset-English](https://huggingface.co/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "!curl -L \"https://huggingface.co/api/datasets/ErfanMoosaviMonazzah/fake-news-detection-dataset-English/parquet/default/train/0.parquet\" -o 0.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the Dataset\n",
        "df = pd.read_parquet(\"0.parquet\")\n",
        "# Standardize the dataset\n",
        "df[\"features\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "df.drop([\"title\", \"text\", \"Unnamed: 0\", \"date\", \"subject\"], axis=1, inplace=True)\n",
        "df.insert(0, \"features\", df.pop(\"features\"))\n",
        "# Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm *.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [mohammadjavadpirhadi/fake-news-detection-dataset-english](https://huggingface.co/datasets/mohammadjavadpirhadi/fake-news-detection-dataset-english)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl -L \"https://huggingface.co/api/datasets/mohammadjavadpirhadi/fake-news-detection-dataset-english/parquet/default/train/0.parquet\" -o 0.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the Dataset\n",
        "df = pd.read_parquet(\"0.parquet\")\n",
        "# Standardize the dataset\n",
        "df[\"features\"] = df[\"title\"] + \" \" + df[\"text\"]\n",
        "df.drop([\"title\", \"text\", \"date\", \"subject\"], axis=1, inplace=True)\n",
        "df.insert(0, \"features\", df.pop(\"features\"))\n",
        "df[\"label\"] = df[\"label\"].apply(lambda x: 1 if x == 0 else 0)\n",
        "\n",
        "# Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm *.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [pushpdeep/fake_news_combined](https://huggingface.co/datasets/pushpdeep/fake_news_combined)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl -L \"https://huggingface.co/api/datasets/pushpdeep/fake_news_combined/parquet/default/train/0.parquet\" -o 0.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the Dataset\n",
        "df = pd.read_parquet(\"0.parquet\")\n",
        "# Standardize the dataset\n",
        "df[\"features\"] = df[\"text\"]\n",
        "df.drop([\"Unnamed: 0\", \"text\"], axis=1, inplace=True)\n",
        "df.insert(0, \"features\", df.pop(\"features\"))\n",
        "\n",
        "# # Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm *.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### [argilla/news-fakenews](https://huggingface.co/datasets/argilla/news-fakenews)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!curl -L \"https://huggingface.co/api/datasets/argilla/news-fakenews/parquet/default/train/0.parquet\" -o 0.parquet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read the Dataset\n",
        "df = pd.read_parquet(\"0.parquet\")\n",
        "# Standardize the dataset\n",
        "df = df[[\"text\",\"prediction\"]]\n",
        "df = df[df[\"text\"] != ' ']\n",
        "df[\"prediction\"] = df[\"prediction\"].apply(lambda x: 1 if x[0][\"label\"] == 'real' else 0)\n",
        "df.columns = [\"features\", \"label\"]\n",
        "# Add the dataset to the list\n",
        "DATASET.append(df)\n",
        "# Show\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm *.parquet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Join all the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.concat(DATASET, ignore_index=True)\n",
        "df = df.sample(frac=1).reset_index(drop=True) # Shuffle the dataset\n",
        "df.to_csv(\"train.csv\", index=False)\n",
        "del DATASET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPOjdos8yuPr"
      },
      "source": [
        "## Inspeccionar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "evNpSbVeyuPs",
        "outputId": "86460e6e-f786-4f72-b852-e0a929c87211"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmuowRWiyuPs",
        "outputId": "48ea296c-4a30-4779-89f8-20af101eae22"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1nzgnNQyuPt"
      },
      "source": [
        "# Visualizacion del Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2TSLYt9yuPt"
      },
      "source": [
        "## Dataset Balanceado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "id": "vfwsz02syuPt",
        "outputId": "00a41b7b-ad6a-48bb-a149-24d040cda8ff"
      },
      "outputs": [],
      "source": [
        "## Librerias para graficaci√≥n\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Visualizamos si la data esta balanceada\n",
        "sns.catplot(x=\"label\", kind=\"count\", color=\"r\", data=df)\n",
        "plt.title(\"Distribuci√≥n de Clasificaci√≥n\")\n",
        "plt.xlabel(\"is True\")\n",
        "plt.ylabel(\"Conteo\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_q19AWaRyuPt"
      },
      "source": [
        "## Word Cloud de los titulares de las *Fake News*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        },
        "id": "hFcVvnP8yuPt",
        "outputId": "c2257bad-f995-4326-eabe-da326ddc2bff"
      },
      "outputs": [],
      "source": [
        "# importing all necessary modules\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "comment_words = \"\"\n",
        "stopwords = set(STOPWORDS)\n",
        "# Recorrer el dataframe\n",
        "for val in df[df[\"label\"] == 0][\"features\"][:100]:\n",
        "    # Castear cada palabra a string\n",
        "    val = str(val)\n",
        "    # Separamos por palabra\n",
        "    tokens = val.split()\n",
        "    # Pasamos a minusculas\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "    comment_words += \" \".join(tokens) + \" \"\n",
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=800,\n",
        "    background_color=\"white\",\n",
        "    stopwords=stopwords,\n",
        "    min_font_size=10,\n",
        ").generate(comment_words)\n",
        "# Generamos el plot\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# importing all necessary modules\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "comment_words = \"\"\n",
        "stopwords = set(STOPWORDS)\n",
        "# Recorrer el dataframe\n",
        "for val in df[df[\"label\"] == 1][\"features\"][:100]:\n",
        "    # Castear cada palabra a string\n",
        "    val = str(val)\n",
        "    # Separamos por palabra\n",
        "    tokens = val.split()\n",
        "    # Pasamos a minusculas\n",
        "    for i in range(len(tokens)):\n",
        "        tokens[i] = tokens[i].lower()\n",
        "    comment_words += \" \".join(tokens) + \" \"\n",
        "wordcloud = WordCloud(\n",
        "    width=800,\n",
        "    height=800,\n",
        "    background_color=\"white\",\n",
        "    stopwords=stopwords,\n",
        "    min_font_size=10,\n",
        ").generate(comment_words)\n",
        "# Generamos el plot\n",
        "plt.figure(figsize=(8, 8), facecolor=None)\n",
        "plt.imshow(wordcloud)\n",
        "plt.axis(\"off\")\n",
        "plt.tight_layout(pad=0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b31APXRLyuPu"
      },
      "source": [
        "# Pre-Procesado de los datos para NLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "# shuffle data\n",
        "df = df.sample(frac=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "'''\n",
        "* processing_text\n",
        "* @param texto str\n",
        "* @return processed_feature str\n",
        "'''\n",
        "def processing_text(texto):\n",
        "\n",
        "    # 1. Limpiar textos\n",
        "    processed_feature = re.sub(r'[^a-zA-Z0-9 ]', '', str(texto)) # Remover con un expresi√≥n regular carateres especiales (no palabras).\n",
        "    processed_feature= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', processed_feature) # Remover ocurrencias de caracteres individuales\n",
        "    processed_feature = re.sub(r'\\^[a-zA-Z]\\s+', ' ', processed_feature) # Remover ocurrencias de caracteres individuales\n",
        "    processed_feature = re.sub(r'[0-9]+', ' ', processed_feature) #  Remover n√∫meros (Ocurrencias muy espor√°dicas en nuestro dataset)\n",
        "    processed_feature = re.sub(' +', ' ', processed_feature) # Simplificar espacios concecutivos a un √∫nico espacio entre palabras\n",
        "    processed_feature = processed_feature.lower() # Pasar todo el texto a min√∫sculas\n",
        "    \n",
        "    # 2. Stop words & lemmatization\n",
        "    from nltk.stem import WordNetLemmatizer\n",
        "    from nltk.corpus import stopwords\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    processed_feature = processed_feature.split()\n",
        "    processed_feature = ' '.join([lemmatizer.lemmatize(word) for word in processed_feature if word not in stop_words])\n",
        "\n",
        "    return processed_feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df[\"features\"] = df[\"features\"].apply(processing_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.to_json(\"features.json\",orient='records', lines=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
