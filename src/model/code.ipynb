{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análisis de datos exploratorio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    !curl -L \"https://prod-dcd-datasets-cache-zipfiles.s3.eu-west-1.amazonaws.com/945z9xkc8d-1.zip\" -o data.zip \n",
    "    !unzip \"data.zip\" && unzip \"Fake News Dataset.zip\"\n",
    "    !rm -r *.zip \n",
    "    \n",
    "    main_directory = os.path.join(os.getcwd(),\"Fake News Dataset\")\n",
    "    subdirectories = [x[0] for x in os.walk(main_directory)][1:]\n",
    "    print(main_directory)\n",
    "    dataframes = []\n",
    "    # Itera a través de los subdirectorios y archivos train.csv\n",
    "    for subdirectory in subdirectories:\n",
    "        file_path = os.path.join(subdirectory, \"train.csv\")\n",
    "        \n",
    "        # Verifica si el archivo train.csv existe en el subdirectorio\n",
    "        if os.path.exists(file_path):\n",
    "            data = pd.read_csv(file_path)\n",
    "            dataframes.append(data)\n",
    "\n",
    "    # Combina los DataFrames en uno solo\n",
    "    combined_data = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "    # Guarda el DataFrame combinado en un archivo train_combined.csv\n",
    "    combined_data.to_csv(\"dataset.csv\", index=False)\n",
    "    print(\"Archivos train.csv combinados con éxito en train_combined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv(\"dataset.csv\",sep=\";\")\n",
    "except:\n",
    "    print(\"Downloading dataset .....\")\n",
    "    load_data()\n",
    "    df = pd.read_csv(\"dataset.csv\",sep=\";\")\n",
    "    # !curl -L \"https://huggingface.co/datasets/GonzaloA/fake_news/resolve/refs%2Fconvert%2Fparquet/default/train/0000.parquet\" -o 0.parquet "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspeccionar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizacion del Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Balanceado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Librerias para graficación\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Visualizamos si la data esta balanceada\n",
    "sns.catplot(x=\"label\", kind=\"count\", color=\"r\", data=df)\n",
    "plt.title(\"Distribución de Clasificación\")\n",
    "plt.xlabel(\"is Fake\")\n",
    "plt.ylabel(\"Conteo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Cloud de los titulares de las *Fake News*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all necessary modules\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "comment_words = \"\"\n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "# Recorrer el dataframe\n",
    "for val in df[\"text\"][0:1]:\n",
    "    # Castear cada palabra a string\n",
    "    val = str(val)\n",
    "\n",
    "    # Separamos por palabra\n",
    "    tokens = val.split()\n",
    "\n",
    "    # Pasamos a minusculas\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = tokens[i].lower()\n",
    "\n",
    "    comment_words += \" \".join(tokens) + \" \"\n",
    "\n",
    "wordcloud = WordCloud(\n",
    "    width=800,\n",
    "    height=800,\n",
    "    background_color=\"white\",\n",
    "    stopwords=stopwords,\n",
    "    min_font_size=10,\n",
    ").generate(comment_words)\n",
    "\n",
    "# Generamos el plot\n",
    "plt.figure(figsize=(8, 8), facecolor=None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-Procesado de los datos para NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"features\"] = df[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importamos word_tokenize desde nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasar el texto de la cadena a la palabra tokenize para romper las oraciones\n",
    "df[\"features\"] = df[\"features\"].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"features\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(FreqDist(df[\"features\"][:1].to_list()[0]).most_common)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.stem.WordNetLemmatizer()\n",
    "nltk.download('wordnet')\n",
    " \n",
    "df[\"features\"] = df[\"features\"].apply(lambda x: [lemmer.lemmatize(y) for y in x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "# debemos indicar que la oracion se encuentra en ingles\n",
    "a = set(stopwords.words(\"english\"))\n",
    "\n",
    "# convertimos el texto a minusculas\n",
    "df[\"features\"] = df[\"features\"].apply(lambda x: [y.lower() for y in x])\n",
    "# Eliminamos las stop words del dataset\n",
    "df[\"features\"] = df[\"features\"].apply(lambda x: [y for y in x if y not in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json = df[\"features\"].to_json(orient=\"values\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de la representacion del texto (Word2Vec, Keras Embedding, BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df[\"features\"])\n",
    "\n",
    "X_sequences = tokenizer.texts_to_sequences(\n",
    "    df[\"features\"]\n",
    ")  # Convertir texto a secuencia de números\n",
    "\n",
    "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_padded, df[\"label\"], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes Neuronales "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten \n",
    "from tensorflow.keras.layers import Embedding, LSTM\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construir y entrenar la red neuronal\n",
    "model = Sequential()\n",
    "\n",
    "# Hiperparámetro de regularización L2\n",
    "l2_lambda = 0.01  # Ajusta este valor según tus necesidades\n",
    "\n",
    "embedding_dim = 100\n",
    "max_words = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Capa de Embedding con regularización L2\n",
    "model.add(\n",
    "    Embedding(\n",
    "        input_dim=max_words,\n",
    "        output_dim=embedding_dim,\n",
    "        input_length=max_sequence_length,\n",
    "        embeddings_regularizer=l2(l2_lambda),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Capa LSTM\n",
    "model.add(LSTM(32))\n",
    "\n",
    "# Capa densa con regularización L2\n",
    "model.add(Dense(32, activation=\"relu\", kernel_regularizer=l2(l2_lambda)))\n",
    "\n",
    "# Capa densa adicional\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "\n",
    "# Capa de salida para la clasificación binaria\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    np.array(X_train),\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    batch_size=64,\n",
    "    verbose=True,\n",
    "    validation_data=(np.array(X_test), y_test),\n",
    "    workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontract(text):\n",
    "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    return text\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    stemmer = PorterStemmer()\n",
    "    stopwords_english = stopwords.words('english')\n",
    "    # remove stock market tickers like $GE\n",
    "    text = re.sub(r'\\$\\w*', '', text)\n",
    "    # remove old style retweet text \"RT\"\n",
    "    text = re.sub(r'^RT[\\s]+', '', text)\n",
    "    # remove hyperlinks\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
    "    # remove hashtags\n",
    "    text = re.sub(r'#', '', text)\n",
    "    text = str(re.sub(\"\\S*\\d\\S*\", \"\", text).strip()) \n",
    "    text = decontract(text)\n",
    "\n",
    "    # tokenize texts\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    \n",
    "    texts_clean = []\n",
    "    for word in tokens:\n",
    "        if (word not in stopwords_english and  # remove stopwords\n",
    "                word not in string.punctuation+'...'):  # remove punctuation\n",
    "            # \n",
    "            stem_word = lemmatizer.lemmatize(word,\"v\")  # Lemmatizing word\n",
    "            texts_clean.append(stem_word)\n",
    "\n",
    "    return \" \".join(texts_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lime.lime_text import LimeTextExplainer\n",
    "class_names=['NotFake','IsFake']\n",
    "explainer= LimeTextExplainer(class_names=class_names)\n",
    "\n",
    "def predict_proba(arr):\n",
    "  processed=[]\n",
    "  for i in arr:\n",
    "    processed.append(process_text(i))\n",
    "  list_tokenized_ex = tokenizer.texts_to_sequences(processed)\n",
    "  Ex = pad_sequences(list_tokenized_ex, maxlen=max_sequence_length)\n",
    "  pred=model.predict(Ex)\n",
    "  returnable=[]\n",
    "  for i in pred:\n",
    "    temp=i[0]\n",
    "    returnable.append(np.array([1-temp,temp])) #I would recommend rounding temp and 1-temp off to 2 places\n",
    "  return np.array(returnable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Actual rating\",df['label'][2])\n",
    "explainer.explain_instance(df['text'][2],predict_proba).show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo\n",
    "loss, accuracy = model.evaluate(np.array(X_test), y_test)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(np.array(X_test))\n",
    "predictions = list(map(lambda x: 1 if (x > 0.5) else 0, predictions))\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions,labels=[0,1])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, list(predictions), digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
