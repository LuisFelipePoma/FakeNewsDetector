{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh1SOqFVyuPo"
      },
      "source": [
        "# Análisis de datos exploratorio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juw3GXrVyuPp"
      },
      "source": [
        "## Cargar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Eh75So8yuPq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"features.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPOjdos8yuPr"
      },
      "source": [
        "## Inspeccionar datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "evNpSbVeyuPs",
        "outputId": "86460e6e-f786-4f72-b852-e0a929c87211"
      },
      "outputs": [],
      "source": [
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OmuowRWiyuPs",
        "outputId": "48ea296c-4a30-4779-89f8-20af101eae22"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89c29vbJyuPw"
      },
      "source": [
        "## Creacion de la representacion del texto (Word2Vec, Keras Embedding, BOW)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Descarga los vectores Word2Vec preentrenados en Google News (esto puede llevar tiempo)\n",
        "word_vectors = api.load(\"word2vec-google-news-300\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZYLpUj_yuPw"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "max_words = 10000\n",
        "max_sequence_length = 250\n",
        "embedding_dim = 250\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words)  # max_words es la cantidad máxima de palabras que deseas considerar\n",
        "tokenizer.fit_on_texts(df[\"features\"])  # Ajusta el tokenizer a tus datos de texto\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "# Inicializa la matriz de embedding con los vectores preentrenados\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in word_vectors:\n",
        "        embedding_matrix[i] = word_vectors[word]\n",
        "\n",
        "        \n",
        "X_sequences = tokenizer.texts_to_sequences(\n",
        "    df[\"features\"]\n",
        ")  # Convertir texto a secuencia de números\n",
        "\n",
        "X_padded = pad_sequences(X_sequences, maxlen=max_sequence_length)\n",
        "X_train, y_train =train_test_split( X_padded, df[\"label\"], test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnHKXsaPgqox"
      },
      "outputs": [],
      "source": [
        "tokenizer_json = tokenizer.index_word\n",
        "inv_map = {v: k for k, v in tokenizer_json.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hi3TgcOpUq8m",
        "outputId": "d40be96b-51c1-49b4-a75a-55b2314821c1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Nombre del archivo de salida\n",
        "nombre_archivo = 'vocab.json'\n",
        "\n",
        "# Guarda el vocabulario invertido en un archivo JSON\n",
        "with open(nombre_archivo, 'w') as archivo:\n",
        "    json.dump(inv_map, archivo)\n",
        "\n",
        "print(f'Vocabulario invertido guardado en {nombre_archivo}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5EKFH80yuPw"
      },
      "source": [
        "# Modelating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Uv5NKAmyuPw"
      },
      "source": [
        "## Redes Neuronales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgiVrYgXyuPw"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nc0WEPP_yuPw",
        "outputId": "608c97dc-f445-4b85-895a-b990151e31ff"
      },
      "outputs": [],
      "source": [
        "# Construir y entrenar la red neuronal\n",
        "model = Sequential()\n",
        "\n",
        "# Capa de Embedding con regularización L2\n",
        "model.add(\n",
        "    Embedding(\n",
        "    input_dim=max_words,\n",
        "    output_dim=embedding_dim,\n",
        "    weights=[embedding_matrix],\n",
        "    input_length=max_sequence_length,\n",
        "    trainable=False\n",
        "    )\n",
        ")\n",
        "\n",
        "# Capa LSTM Bidireccional con regularización L2\n",
        "model.add(Bidirectional(LSTM(128)))\n",
        "\n",
        "# Capa de Dropout para regularización\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Capa densa con regularización L2\n",
        "model.add(Dense(128, activation=\"relu\"))\n",
        "\n",
        "# Otra capa de Dropout\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "# Capa de salida sigmoide\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Or-fTU1syuPx",
        "outputId": "bbd8aee1-551c-4397-eaaf-928d72ae80ef"
      },
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    np.array(X_train),\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=128,\n",
        "    verbose=True,\n",
        "    validation_data=(np.array(X_test), y_test),\n",
        "    workers=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MT0PSnTyyuPx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRJ5yqEAyuPx"
      },
      "outputs": [],
      "source": [
        "def decontract(text):\n",
        "    text = re.sub(r\"won\\'t\", \"will not\", text)\n",
        "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
        "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'s\", \" is\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'t\", \" not\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'m\", \" am\", text)\n",
        "    return text\n",
        "lemmatizer = WordNetLemmatizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3TbFPNpyuPx"
      },
      "outputs": [],
      "source": [
        "def process_text(text):\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    # remove stock market tickers like $GE\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    # remove old style retweet text \"RT\"\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    # remove hyperlinks\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    # remove hashtags\n",
        "    text = re.sub(r'#', '', text)\n",
        "    text = str(re.sub(\"\\S*\\d\\S*\", \"\", text).strip())\n",
        "    text = decontract(text)\n",
        "\n",
        "    # tokenize texts\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    texts_clean = []\n",
        "    for word in tokens:\n",
        "        if (word not in stopwords_english and  # remove stopwords\n",
        "                word not in string.punctuation+'...'):  # remove punctuation\n",
        "            #\n",
        "            stem_word = lemmatizer.lemmatize(word,\"v\")  # Lemmatizing word\n",
        "            texts_clean.append(stem_word)\n",
        "\n",
        "    return \" \".join(texts_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCWG5D064L0i"
      },
      "outputs": [],
      "source": [
        "%pip install lime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRdiuk6LyuPx"
      },
      "outputs": [],
      "source": [
        "from lime.lime_text import LimeTextExplainer\n",
        "class_names=['NotFake','IsFake']\n",
        "explainer= LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "def predict_proba(arr):\n",
        "  processed=[]\n",
        "  for i in arr:\n",
        "    processed.append(process_text(i))\n",
        "  list_tokenized_ex = tokenizer.texts_to_sequences(processed)\n",
        "  Ex = pad_sequences(list_tokenized_ex, maxlen=max_sequence_length)\n",
        "  pred=model.predict(Ex)\n",
        "  returnable=[]\n",
        "  for i in pred:\n",
        "    temp=i[0]\n",
        "    returnable.append(np.array([1-temp,temp])) #I would recommend rounding temp and 1-temp off to 2 places\n",
        "  return np.array(returnable)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6pzin3wcyuPy"
      },
      "outputs": [],
      "source": [
        "print(\"Actual rating\",df['label'][2])\n",
        "explainer.explain_instance(df['text'][2],predict_proba).show_in_notebook(text=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Bolsa de palabras\n",
        "vectorizer = CountVectorizer(max_features=5000, stop_words=stopwords.words('english'))\n",
        "\n",
        "# Ahora le solicitamos utilizando nuestro conjunto de datos que construya el vocabulario y tambien transforme nuestro texto\n",
        "texto_features = vectorizer.fit_transform(df['features'][0])\n",
        "\n",
        "palabras = vectorizer.get_feature_names_out()\n",
        "\n",
        "frecuencias = texto_features.toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "frecuencias_totales = frecuencias.sum(axis=0)\n",
        "\n",
        "# Obtener las palabras más frecuentes y sus frecuencias\n",
        "palabras_mas_frecuentes = [palabras[i] for i in frecuencias_totales.argsort()[::-1][:10]]\n",
        "frecuencias_mas_frecuentes = [frecuencias_totales[i] for i in frecuencias_totales.argsort()[::-1][:10]]\n",
        "\n",
        "# Crear un gráfico de barras horizontal\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(palabras_mas_frecuentes, frecuencias_mas_frecuentes, color='skyblue')\n",
        "plt.xlabel('Frecuencia')\n",
        "plt.ylabel('Palabra')\n",
        "plt.title('Palabras más frecuentes')\n",
        "plt.gca().invert_yaxis()  # Invertir el eje y para mostrar las palabras más frecuentes arriba\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNyaYIQJyuPy"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xLu3qGYtyuP8"
      },
      "outputs": [],
      "source": [
        "# Evaluar el modelo\n",
        "loss, accuracy = model.evaluate(np.array(X_test), y_test)\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oovCAhafyuP8"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzbg7b2gyuP8"
      },
      "outputs": [],
      "source": [
        "predictions = model.predict(np.array(X_test))\n",
        "predictions = list(map(lambda x: 1 if (x > 0.5) else 0, predictions))\n",
        "\n",
        "cm = confusion_matrix(y_test, predictions,labels=[0,1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0,1])\n",
        "disp.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dwayw6XkyuP9"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_test, list(predictions), digits=4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qb5NX9v0yuP9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(history.history[\"loss\"])\n",
        "plt.plot(history.history[\"val_loss\"])\n",
        "plt.title(\"model loss\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNxUYmAPztct"
      },
      "outputs": [],
      "source": [
        "%pip install skl2onnx\n",
        "%pip install tensorflowjs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ytWx-nazt-6"
      },
      "outputs": [],
      "source": [
        "import tensorflowjs as tfjs\n",
        "tfjs.converters.save_keras_model(model, \"rnn_model\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
